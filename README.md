"# 크롤러"

## 크롤러

사이트에 있는 정보 수집

# 작동순서(에어플로우)

1. 에어플로우에서 spider 파일 실행
2. 원하는 사이트 정보 수집
3. 정보 수집 후 가공해서 collection update 및 데이터 편하게 볼 수 있도록 google sheet에 전송
4. google sheet에 해당 데이터 및 최근 날짜의 데이터만 보여주기

참고

- scrapy으로 만들어진 크롤러 위치: spiders 폴더 확인
